## 准备

tmux用于窗口和实验进程分离

https://www.ruanyifeng.com/blog/2019/10/tmux.html

### 环境

```shell
# 创建虚拟环境
conda create -n langgpt python=3.10 -y
```

运行下面的命令，激活虚拟环境：

```shell
conda activate langgpt
```

之后的操作都要在这个环境下进行。激活环境后，安装必要的Python包，依次运行下面的命令：

```shell
# 安装一些必要的库
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y

# 安装其他依赖
pip install transformers==4.43.3

pip install streamlit==1.37.0
pip install huggingface_hub==0.24.3
pip install openai==1.37.1
pip install lmdeploy==0.5.2
```

### 项目路径

运行如下命令创建并打开项目路径：

```shell
## 创建路径
mkdir langgpt
## 进入项目路径
cd langgpt
```

## 模型部署

这部分基于LMDeploy将开源的InternLM2-chat-1_8b模型部署为OpenAI格式的通用接口。

### 获取模型

- 如果使用intern-studio开发机，可以直接在路径`/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`下找到模型

- 如果不使用开发机，可以从huggingface上获取模型，地址为：https://huggingface.co/internlm/internlm2-chat-1_8b

  可以使用如下脚本下载模型：

  ```python
  from huggingface_hub import login, snapshot_download
  import os
  
  os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
  
  login(token=“your_access_token")
  
  models = ["internlm/internlm2-chat-1_8b"]
  
  for model in models:
      try:
          snapshot_download(repo_id=model,local_dir="langgpt/internlm2-chat-1_8b")
      except Exception as e:
          print(e)
          pass
  ```

### 部署模型为OpenAI server

由于服务需要持续运行，需要将进程维持在后台，所以这里使用`tmux`软件创建新的命令窗口。运行如下命令创建窗口：

```shell
tmux new -t langgpt
```



创建完成后，运行下面的命令进入新的命令窗口(首次创建自动进入，之后需要连接)：

```shell
tmux a -t langgpt
```

进入命令窗口后，需要在新窗口中再次激活环境，命令参考**0.1节**。然后，使用LMDeploy进行部署，参考如下命令：

使用LMDeploy进行部署，参考如下命令：

```shell
CUDA_VISIBLE_DEVICES=0 lmdeploy serve api_server /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --server-port 23333 --api-keys internlm2
```

更多设置，可以参考：https://lmdeploy.readthedocs.io/en/latest/index.html

部署成功后，可以利用如下脚本调用部署的InternLM2-chat-1_8b模型并测试是否部署成功。

```python
from openai import OpenAI

client = OpenAI(
    api_key = "internlm2",
    base_url = "http://0.0.0.0:23333/v1"
)

response = client.chat.completions.create(
    model=client.models.list().data[0].id,
    messages=[
        {"role": "system", "content": "请介绍一下你自己"}
    ]
)

print(response.choices[0].message.content)
```

服务启动完成后，可以按Ctrl+B进入`tmux`的控制模式，然后按D退出窗口连接，更多操作[参考](https://aik9.top/)。

### 图形化界面调用

InternLM部署完成后，可利用提供的`chat_ui.py`创建图形化界面，在实战营项目的tools项目中。

首先，从Github获取项目，运行如下命令：

```
git clone https://github.com/InternLM/Tutorial.git
```

下载完成后，运行如下命令进入项目所在的路径：

```
cd Tutorial/tools
```

进入正确路径后，运行如下脚本运行项目：

```
python -m streamlit run chat_ui.py
```

参考[L0/Linux的2.3部分](https://github.com/InternLM/Tutorial/tree/camp3/docs/L0/Linux#23-端口映射)进行端口映射，在本地终端中输入映射命令，可以参考如下命令：

```shell
ssh -p {ssh端口，从InternStudio获取} root@ssh.intern-ai.org.cn -CNg -L 7860:127.0.0.1:8501 -o StrictHostKeyChecking=no
```

如果未配置开发机公钥，还需要输入密码，从InternStudio获取。上面这一步是将开发机上的8501(web界面占用的端口)映射到本地机器的端口，之后可以访问http://localhost:7860/打开界面。

## 任务

- **背景问题**：近期相关研究发现，LLM在对比浮点数字时表现不佳，经验证，internlm2-chat-1.8b (internlm2-chat-7b)也存在这一问题，例如认为`13.8<13.11`。
- **任务要求**：利用LangGPT优化提示词，使LLM输出正确结果。

![](https://cdn.jsdelivr.net/gh/Moyu-moyuing/ImageHostingWebsite@main/Img/20240921215124.png)

## 提示词

```markdown
# Role: 数学逻辑专家

## Background:  
我是一名精通数学逻辑和数值比较的专家。无论是整数还是浮点数，我都能够基于标准的数学规则做出正确的判断，确保所有计算结果的准确性。

## Attention:
在比较数字大小时，必须严格按照数学定义，特别是在处理浮点数时，小数点后的数位至关重要。任何形式的数值比较都应遵循严格的数学逻辑，以确保准确无误的结果。

## Profile:  
- 姓名: 数学逻辑专家
- 专业领域: 数学、逻辑分析、数值比较
- 座右铭: 以精确为准则

### Skills:
- 深入理解浮点数和整数的数学特性
- 具备严谨的数值比较能力，确保不出错
- 熟练处理复杂的数值运算和逻辑判断

## Goals:  
- 通过标准的数学规则，确保所有数字比较的结果准确无误
- 特别注意浮点数的处理，避免错误判断
- 在数字比较中提供清晰、可靠的解释

## Constraints:  
- 不能忽视浮点数的小数部分，必须严格按照数学规则进行大小比较
- 避免主观推测，确保所有判断都基于数值本身

## Workflow:
1. 接受两个或多个数值（包括整数和浮点数）
2. 通过比较整数部分和小数部分，严格按照数学逻辑判断大小
3. 输出明确的大小关系，特别是在浮点数对比时，确保准确

## OutputFormat:  
- 逻辑严谨、清晰明确的大小判断
- 特别注重浮点数的正确处理和解释
- 避免任何形式的误判

## Initialization:
在进行数值比较时，请遵循以下基本规则：数字的大小关系必须基于其精确值，而不能依赖直观感受。确保每次判断符合数学常识和规则，严谨处理每一个数字，特别是浮点数。


```

